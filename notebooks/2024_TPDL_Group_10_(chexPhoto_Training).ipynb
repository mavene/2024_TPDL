{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"29fa679ffee74ebcae563b647b03cb96","deepnote_cell_type":"markdown","id":"P7Wnon2hSjX4"},"source":["# 50.039 Theory and Practice of Deep Learning Project 2024"]},{"cell_type":"markdown","metadata":{"cell_id":"e9702732b5d94f17bdda5c41609e3d4c","deepnote_cell_type":"markdown","id":"8pDddTCFTJ5e"},"source":["Group 10\n","- Issac Jose Ignatius (1004999)\n","- Mahima Sharma (1006106)\n","- Dian Maisara (1006377)\n"]},{"cell_type":"markdown","metadata":{"cell_id":"5115ec45191f42009894d091bcd6b1d9","deepnote_cell_type":"markdown","id":"dYA9XwWzT1eq"},"source":["## Motivation"]},{"cell_type":"markdown","metadata":{"cell_id":"4298c214d28c4b8fa5b7dbb347e039e9","deepnote_cell_type":"markdown","id":"Jym9IzWeTp_t"},"source":["Chest radiography is an essential diagnostic tool used in medical imaging to visualise structures and organs within the chest cavity. It is crucial for diagnosing various respiratory and heart-related conditions. However, with the increased demand for radiological reports within shorter timeframes to detect and treat illnesses, there have been insufficient radiologists available to perform such tasks at scale. Therefore, automated chest radiograph interpretation could provide substantial benefits supporting large-scale screening and population health initiatives. Deep-learning algorithms can be used to bridge this gap. They have been used for image classification, anomaly detection, organ segmentation, and disease progression prediction.\n","\n","*In this project, we aim to train a deep neural network to perform multi-label image classification on a wide array of chest radiograph images that exhibit various pathologies.*<br><br>\n","\n","---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"5061937273004b9e8b4f895d7115cfe2","deepnote_cell_type":"markdown","id":"vca9P0PG41JP"},"source":["## Import all relevant libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install --upgrade torchmetrics\n","%pip install --upgrade tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"5c8171dbd74f4c2b84ec17c418017814","colab":{"base_uri":"https://localhost:8080/"},"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":17522,"execution_start":1711290010055,"id":"zhwXTMVh444O","outputId":"c8fd5395-9195-4673-9cba-ed3298fddd1b","source_hash":null},"outputs":[],"source":["# Numpy\n","import numpy as np\n","# Pandas\n","import pandas as pd\n","\n","# Torch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, RandomSampler\n","import torchvision\n","print(torchvision.__version__)\n","import torchvision.transforms as T\n","from torchvision.transforms import v2\n","import torchvision.transforms.functional as fn\n","from torchvision.io import read_image, ImageReadMode\n","\n","# Image\n","# import PIL\n","# from PIL import ImageFile\n","\n","# File Operations\n","from glob import glob\n","import os\n","\n","# Helper scripts\n","# import sys\n","# sys.path.insert(0, '../src')\n","# from saver_loader import *\n","# %reload_ext autoreload\n","# %autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gc\n","\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use GPU if available, else use CPU\n","device = torch.device(\"cuda\") # if torch.cuda.is_available() else \"cpu\")\n","# device = torch.device(\"cpu\") \n","print(device)"]},{"cell_type":"markdown","metadata":{"cell_id":"182e8d2e6f7d4ced8f8e61c49887467a","deepnote_cell_type":"markdown","id":"Z12EVXOwUKsx"},"source":["## Data Loading"]},{"cell_type":"markdown","metadata":{"cell_id":"7d72f8de55e44825906c89caebc35cf6","deepnote_cell_type":"markdown","id":"Zr9mlGP5UNb_"},"source":["The training and validation datasets are from the **CheXphoto dataset** (Philips et al., 2020). <br><br> CheXphoto comprises a training set of natural photos and synthetic transformations of 10,507 X-ray images from 3,000 unique patients (32,521 data points) sampled at random from the CheXpert training dataset and an accompanying validation set of natural and synthetic transformations applied to all 234 X-ray images from 200 patients with an additional 200 cell phone photos of x-ray films from another 200 unique patients (952 data points)."]},{"cell_type":"markdown","metadata":{"cell_id":"2671f4063cc04bb089ed3352f29165fb","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Setup environmental variables"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"31071756fb3e47d385d81056c8168c19","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":144,"execution_start":1711290027583,"source_hash":null},"outputs":[],"source":["data_path = os.path.join(os.path.abspath(''), \"../ChexPhoto/chexphoto-v1\")\n","print(data_path)"]},{"cell_type":"markdown","metadata":{"cell_id":"8bc124d45d23406f8e03d3a4b05b97fd","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Loading dataset (image and labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"afcb62acc5c747d3b062c5230334e40b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":580,"execution_start":1711290027624,"source_hash":null},"outputs":[],"source":["# Bug in Path present in training dataset\n","def fix_error_paths(row):\n","    row = row.replace(\"//\", \"/\")\n","    return row\n","\n","def str_to_array(row):\n","    row = row.replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \", \"\").split(\".\")[:-1]\n","    return np.array(row).astype(np.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["labels = ['Pleural Effusion', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Edema', 'Pleural Other', 'Fracture', 'Support Devices']\n","\n","train_df = pd.read_csv(\"../data/processed/train_one_hot_encoded.csv\", index_col=False)\n","train_df[\"Path\"] = train_df[\"Path\"].apply(fix_error_paths)\n","valid_df = pd.read_csv(\"../data/processed/valid_one_hot_encoded.csv\", index_col=False)\n","#test_df = pd.read_csv(\"../data/processed/test_one_hot_encoded.csv\", index_col=False)\n","\n","for label in labels:\n","    train_df[label] = train_df[label].apply(str_to_array)\n","    valid_df[label] = valid_df[label].apply(str_to_array)\n","    #test_df[label] = test_df[label].apply(str_to_array)"]},{"cell_type":"markdown","metadata":{"cell_id":"f4537554fa604cbca09cd2462015c3c3","deepnote_app_block_visible":false,"deepnote_cell_type":"text-cell-h3","formattedRanges":[],"is_collapsed":false},"source":["### Visualize dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"5b1d29b4d96f48ad9a7cb320278afa69","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":268,"execution_start":1711290028208,"source_hash":null},"outputs":[],"source":["display(train_df)\n","display(valid_df)\n","#display(test_df)"]},{"cell_type":"markdown","metadata":{"cell_id":"cbf34e591d8547fc8b9c4662c4712d49","deepnote_app_block_visible":false,"deepnote_cell_type":"text-cell-h3","formattedRanges":[],"is_collapsed":false},"source":["### Custom Dataset Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"e9e1926169954d2fa6da5520ee3d1edd","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":166,"execution_start":1711290028614,"source_hash":null},"outputs":[],"source":["# ImageFile.LOAD_TRUNCATED_IMAGES = True\n","# torchvision.set_image_backend('PIL')\n","\n","# Implementation of Custom Dataset Class for CheXPhoto Dataset\n","class CheXDataset(torch.utils.data.Dataset):\n","    # Accepts dataframe object and str\n","    def __init__(self, df: pd.DataFrame): #, split: str):\n","        self.dataframe = df\n","        # Note: self.split is used to identify test set in __getitem__ method due to the arrangement of values in tensor\n","        #self.split = split\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        x_path = data_path + \"/\" + self.dataframe.iloc[idx, 0].split(\"CheXphoto-v1.0\", 1)[-1]\n","        transform = T.Compose([\n","            v2.Resize((512, 512), interpolation=T.InterpolationMode.BICUBIC)#,\n","            #T.ToTensor()\n","        ])\n","        #resized_x_tensor = transform(PIL.Image.open(x_path).convert('RGB'))\n","        resized_x_tensor = transform(read_image(x_path, mode = ImageReadMode.RGB)) /255\n","\n","        # Perform different slicing operations based on the split - focus is on handling test split\n","        # Additional input features e.g. sex, age, FoL, AoP are removed for now\n","        # if self.split.lower() == \"test\":\n","        y_str = self.dataframe.iloc[idx, 1:].to_string(index=False)\n","        y_arr = np.array(y_str.replace(\"[\",\"\").replace(\"]\",\"\").split(\",\"), dtype=np.float32)\n","        y = torch.from_numpy(y_arr)\n","        # else:\n","        #   y = torch.tensor(self.dataframe.iloc[idx, 5:], dtype=torch.float32)\n","\n","        return resized_x_tensor, y"]},{"cell_type":"markdown","metadata":{"cell_id":"c518b378ca804974946d554b10d0d382","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Subsetting dataset (Binary Classification for Pleural Effusion)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4bbf1a91d7a64ad0a8f1b313fb51718a","deepnote_cell_type":"code","deepnote_table_loading":false,"deepnote_table_state":{"filters":[],"pageIndex":0,"pageSize":10,"sortBy":[]},"deepnote_to_be_reexecuted":false,"execution_millis":228,"execution_start":1711290028753,"source_hash":null},"outputs":[],"source":["def preprocess_pEffClassification(df):\n","    df.drop(columns=['Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Edema', 'Pleural Other', 'Fracture', 'Support Devices'], axis=1, inplace=True)\n","    return df\n","\n","# # Drop all other labels\n","pEff_train = preprocess_pEffClassification(train_df)\n","pEff_valid = preprocess_pEffClassification(valid_df)\n","#pEff_test = preprocess_pEffClassification(test_df)\n","\n","display(pEff_train)\n","display(pEff_valid)\n","#display(pEff_test)"]},{"cell_type":"markdown","metadata":{"cell_id":"801469caefd940178748db15f9bb16d4","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### Custom Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"9fa71dc039024dd89cbaebe4693db40d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":174,"execution_start":1711290028934,"source_hash":null},"outputs":[],"source":["# Load into custom Dataset\n","pEff_train_data = CheXDataset(pEff_train)\n","pEff_valid_data = CheXDataset(pEff_valid)\n","#pEff_test_data = CheXDataset(pEff_test, \"test\")\n","\n","# Prepare random sampler for training subset (10% of samples)\n","# pEff_train_sampler = RandomSampler(pEff_train_data, replacement=True, num_samples=int(1*len(edema_train_data))) # 0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7b3d4353f028455f9a0cc387b6114e20","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":162,"execution_start":1711290028947,"source_hash":null},"outputs":[],"source":["# Load into DataLoader\n","batch_size = 32\n","pEff_train_loader = DataLoader(pEff_train_data, batch_size, shuffle=True) #, sampler=edema_train_sampler), pin_memory=True\n","pEff_valid_loader = DataLoader(pEff_valid_data, batch_size)\n","#pEff_test_loader = DataLoader(pEff_test_data, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x, y = pEff_train_data[0]\n","print(x, x.shape, x.dtype)\n","print(y, y.shape, y.dtype)"]},{"cell_type":"markdown","metadata":{"cell_id":"f861868c2bba44619b4c2b13f74497de","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["## Model Tuning"]},{"cell_type":"markdown","metadata":{},"source":["Our initial model is a simple feedforward neural network with multiple heads (14 heads) capable of classifying each observation for the various pathologies. We utilise the Cross-entropy loss function to optimise the model during training.\n","\n","**This is a TODO since it can change**\n"]},{"cell_type":"markdown","metadata":{},"source":["### First Iteration - Convolutional Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"fb0476fb49b44e97bdaf9bce088b9e86","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":50,"execution_start":1711290029081,"source_hash":null},"outputs":[],"source":["# We will be using Convolutional Neural Network\n","class FullCNN(nn.Module):\n","    def __init__(self):\n","        super(FullCNN, self).__init__()\n","\n","        # Conv Layers\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size = 3, stride = 1, padding = 1)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size = 3, stride = 1, padding = 1)\n","        self.conv3 = nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1)\n","        self.conv4 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1)\n","\n","        # BN Layers\n","        self.batch_norm1 = nn.BatchNorm2d(16)\n","        self.batch_norm2 = nn.BatchNorm2d(32)\n","        self.batch_norm3 = nn.BatchNorm2d(64)\n","        self.batch_norm4 = nn.BatchNorm2d(128)\n","\n","        # MP Layer\n","        self.maxpool2d = F.max_pool2d\n","\n","        # FC Layers\n","        self.fc1 = nn.Linear(128*32*32, 8192, dtype = torch.float32)\n","        self.fc2 = nn.Linear(8192, 512, dtype = torch.float32)\n","        self.fc3 = nn.Linear(512, 32, dtype = torch.float32)\n","        self.fc4 = nn.Linear(32, 3, dtype = torch.float32)\n","\n","        # DO Layers\n","        self.dropout = nn.Dropout(0.2)\n","\n","        # TODO: Skip connection implementation\n","        # Skip connection implementation\n","        # self.skip_connection1 = nn.Conv2d(16, 64, kernel_size=1, stride=1)\n","        # self.skip_connection2 = nn.Conv2d(32, 128, kernel_size=1, stride=1)\n","\n","        # Softmax Layer - Not needed when using CrossEntropyLoss\n","        # self.softmax = F.softmax\n","\n","\n","    def forward(self,x):\n","        # All forward operations\n","        x1 = x.to(torch.float32)\n","        # First convolutional layer\n","        x1 = self.conv1(x1)\n","        x1 = self.batch_norm1(x1)\n","        x1 = F.relu(x1)\n","        x1 = self.maxpool2d(x1, 2)\n","        \n","        # Second convolutional layer -> pooling\n","        x2 = self.conv2(x1)\n","        x2 = self.batch_norm2(x2)\n","        x2 = F.relu(x2)\n","        x2 = self.maxpool2d(x2, 2)\n","\n","        # Apply skip connection after the second convolutional layer\n","        # skip_connection_output1 = self.skip_connection1(x1)\n","        # x2 += skip_connection_output1\n","\n","        # Third convolutional layer -> pooling\n","        x3 = self.conv3(x2)\n","        x3 = self.batch_norm3(x3)\n","        x3 = F.relu(x3)\n","        x3 = self.maxpool2d(x3, 2)\n","\n","        # Apply skip connection after the third convolutional layer\n","        # skip_connection_output2 = self.skip_connection2(x2)\n","        # x3 += skip_connection_output2\n","\n","        # Fourth convolutional layer -> pooling\n","        x4 = self.conv4(x3)\n","        x4 = self.batch_norm4(x4)\n","        x4 = F.relu(x4)\n","        x4 = self.maxpool2d(x4, 2)\n","\n","        # print(x.shape)\n","        # Flatten output of convolutions\n","        x4 = x4.view(-1, 128*32*32)\n","        # print(x.shape)\n","        x4 = self.dropout(x4)\n","\n","        # First FC layer\n","        x5 = self.fc1(x4)\n","        x5 = F.relu(x5)\n","        x5 = self.dropout(x5)\n","\n","        # Second FC layer\n","        x6 = self.fc2(x5)\n","        x6 = F.relu(x6)\n","        x6 = self.dropout(x6)\n","\n","        # Third FC layer\n","        x7 = self.fc3(x6)\n","        x7 = F.relu(x7)\n","        x7 = self.dropout(x7)\n","        \n","        # Fourth FC layer\n","        x8 = self.fc4(x7)\n","        #x = self.softmax(x) # No need, Refer to https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n","\n","        return x8"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"fd625ed9ffee4647846b2582f11beba8","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2636,"execution_start":1711290029115,"source_hash":null},"outputs":[],"source":["# Create Model\n","#model = torchvision.models.resnet18(weights=False).to(device)\n","model = FullCNN().to(device)\n","print(model)"]},{"cell_type":"markdown","metadata":{"cell_id":"e64be244b0684e7baa05f881c1cd352c","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["#### Training CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"e74a3a28ec874359b2cb981c0e114551","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1711290031754,"source_hash":null},"outputs":[],"source":["from tqdm.notebook import tnrange, tqdm\n","# torch.cuda.empty_cache()\n","# del model\n","# import gc\n","# gc.collect()\n","def train(model, train_loader, valid_loader, epochs=10, lr=1e-3):\n","    # Adam\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    loss = nn.CrossEntropyLoss().to(device)\n","    # print(accuracy.device)  \n","    #train_loss_values = []\n","    #train_accuracy_values = []\n","    for epoch in tnrange(epochs):\n","        # print(epoch)\n","        model.train()\n","\n","        train_loss = 0.0\n","        train_total = 0\n","        train_correct = 0\n","\n","        for inputs, outputs in tqdm(train_loader):\n","            inputs_re, outputs_re = inputs.to(device), outputs.to(device)\n","            optimizer.zero_grad()\n","            preds = model(inputs_re)\n","            #print([torch.argmax(outputs_re[i], dim=0, keepdim=True) for i in range(batch_size)]) DONT DELETE\n","            #print(torch.stack([torch.argmax(outputs_re[i], dim=0, keepdim=True) for i in range(batch_size)],dim=0)) DONT DELETE\n","            #resized_out = torch.tensor([torch.argmax(outputs_re[i], dim=0, keepdim=True) for i in range(batch_size)]).to(device)\n","            # print(\"1:\",torch.argmax(outputs_re, dim=1))\n","            # print(\"2:\",(torch.argmax(outputs_re, dim=1).reshape(-1,1)))\n","            # print(\"3:\",torch.argmax(outputs_re, dim=1).unsqueeze(1))\n","\n","            loss_value = loss(preds, torch.argmax(outputs_re, dim=1))# Cross Entropy accepts class labels so we need to convert the OHE\n","            loss_value.backward()\n","            optimizer.step()\n","\n","            # Compute metric\n","            train_loss += loss_value.item()\n","            train_total += outputs_re.size(0)\n","            #print(\"Preds argmax\", torch.argmax(preds, dim=1))\n","            #print(\"Outputs_re original argmax\", torch.argmax(outputs_re, dim=1))\n","            train_correct += (torch.argmax(preds, dim=1) == torch.argmax(outputs_re, dim=1)).sum().item() # Convert both to class labels\n","            \n","        # Rewrite implementation for evaluation. Note: Dropout and BatchNorm layers are active when model is in train mode. \n","        # Need to disable them before computing losses and accuracy with model.eval()\n","        train_loss /= len(train_loader)\n","        train_accuracy = train_correct / train_total\n","\n","        model.eval() # handles Dropout (turn off) and BatchNorm (running stats) layers automatically\n","        \n","        val_total = 0\n","        val_correct = 0\n","\n","        with torch.no_grad():\n","            for inputs, outputs in tqdm(valid_loader):\n","                # Retrieve predictions\n","                inputs_re, outputs_re = inputs.to(device), outputs.to(device)\n","                preds = model(inputs_re)\n","\n","                # Change the line to as follows -\n","                #final_preds = torch.where(preds < 0.5, 0, 1)\n","                #final_preds = final_preds.to(device)\n","                # print(torch.argmax(preds, dim=1))\n","                # print(torch.argmax(outputs_re, dim=1))\n","                # print(\"\\n\")\n","\n","                # Compute metrics\n","                val_total += outputs_re.size(0)\n","                val_correct += (torch.argmax(preds, dim=1) == torch.argmax(outputs_re, dim=1)).sum().item() # Convert both to class labels\n","\n","        val_accuracy = val_correct / val_total    \n","        print(f'--- Epoch {epoch+1}/{epochs}: Train loss: {train_loss:.4f}, Train accuracy: {train_accuracy:.4f}\\n Validation accuracy: {val_accuracy}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"ef7b7168c411422290696967882013ea","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":null},"outputs":[],"source":["train(model, pEff_train_loader, pEff_valid_loader)\n","#torch.save(model.state_dict(), 'weights.pt')"]},{"cell_type":"markdown","metadata":{"cell_id":"65c96dd31cba46fba011ec2064f3c98b","deepnote_cell_type":"markdown","id":"iI5olU_kZFMt"},"source":["## Observations"]},{"cell_type":"markdown","metadata":{"cell_id":"4654a8f131c9404db7e34cb8ab64fc0c","deepnote_cell_type":"markdown","id":"cWoXsxGDZSzV"},"source":["**TODO** Discuss whether its right for us to pluck all our evaluation and training together and discuss it here or break up the code without any descriptions\n"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d056a7b8-1929-4f43-a228-a643b0e765c5' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"ae53f212c8234320aeb7433dec39931b","deepnote_persisted_session":{"createdAt":"2024-03-23T01:32:35.460Z"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
