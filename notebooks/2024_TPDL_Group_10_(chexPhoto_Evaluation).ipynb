{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"76ab8bde33b542afb6d0413ffc6bb1f8","deepnote_cell_type":"markdown","id":"P7Wnon2hSjX4"},"source":["# 50.039 Theory and Practice of Deep Learning Project 2024"]},{"cell_type":"markdown","metadata":{"cell_id":"a4e06d30136d405fafb20a665bef99ba","deepnote_cell_type":"markdown","id":"8pDddTCFTJ5e"},"source":["Group 10\n","- Issac Jose Ignatius (1004999)\n","- Mahima Sharma (1006106)\n","- Dian Maisara (1006377)\n"]},{"cell_type":"markdown","metadata":{"cell_id":"abba0498fc274b1dadd617faa0fd896d","deepnote_cell_type":"markdown","id":"vca9P0PG41JP"},"source":["### Import all relevant libraries"]},{"cell_type":"code","execution_count":24,"metadata":{"cell_id":"443bbd1b51054c4b92c17f7c455a9cbb","colab":{"base_uri":"https://localhost:8080/"},"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":6500,"execution_start":1711475524785,"id":"zhwXTMVh444O","outputId":"80c0b9fe-6f28-46b3-9f07-18b3fa99035d","source_hash":null},"outputs":[{"name":"stdout","output_type":"stream","text":["0.17.2+cu121\n"]}],"source":["# Numpy\n","import numpy as np\n","# Pandas\n","import pandas as pd\n","\n","# Torch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","print(torchvision.__version__)\n","import torchvision.transforms as T\n","from torchvision.transforms import v2\n","import torchvision.transforms.functional as fn\n","from torchvision.io import read_image, ImageReadMode\n","from torchmetrics.classification import MulticlassAccuracy\n","import math\n","\n","# File Operations\n","import os\n","\n","# import sys\n","# sys.path.insert(0, '../src')\n","# from saver_loader import *\n","# %reload_ext autoreload\n","# %autoreload 2"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["# Use GPU if available, else use CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# device = torch.device(\"cpu\") \n","print(device)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["c:\\Users\\dianm\\Downloads\\Dataset\\2024_TPDL\\notebooks\\../../original\n"]}],"source":["data_path = os.path.join(os.path.abspath(''), \"../../original\")\n","print(data_path)"]},{"cell_type":"markdown","metadata":{},"source":["### Load in Labels "]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# Bug in Path present in training dataset\n","def fix_error_paths(row):\n","    row = row.replace(\"//\", \"/\")\n","    return row\n","\n","def str_to_array(row):\n","    ndarray = np.fromstring(\n","                row.replace('\\n','')\n","                    .replace('[','')\n","                    .replace(']','')\n","                    .replace('  ',' '), \n","                    sep=' ')\n","    return ndarray"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Path</th>\n","      <th>Enlarged Cardiomediastinum</th>\n","      <th>Cardiomegaly</th>\n","      <th>Lung Opacity</th>\n","      <th>Lung Lesion</th>\n","      <th>Edema</th>\n","      <th>Consolidation</th>\n","      <th>Pneumonia</th>\n","      <th>Atelectasis</th>\n","      <th>Pneumothorax</th>\n","      <th>Pleural Effusion</th>\n","      <th>Pleural Other</th>\n","      <th>Fracture</th>\n","      <th>Support Devices</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>test/patient64741/study1/view1_frontal.jpg</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>test/patient64742/study1/view1_frontal.jpg</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>test/patient64743/study1/view1_frontal.jpg</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>test/patient64744/study1/view1_frontal.jpg</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>test/patient64744/study1/view2_lateral.jpg</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>663</th>\n","      <td>test/patient65236/study1/view1_frontal.jpg</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","    </tr>\n","    <tr>\n","      <th>664</th>\n","      <td>test/patient65237/study1/view1_frontal.jpg</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>665</th>\n","      <td>test/patient65238/study1/view1_frontal.jpg</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","    </tr>\n","    <tr>\n","      <th>666</th>\n","      <td>test/patient65239/study1/view1_frontal.jpg</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","    </tr>\n","    <tr>\n","      <th>667</th>\n","      <td>test/patient65240/study1/view1_frontal.jpg</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 1.0, 0.0]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>668 rows × 14 columns</p>\n","</div>"],"text/plain":["                                           Path Enlarged Cardiomediastinum  \\\n","0    test/patient64741/study1/view1_frontal.jpg            [0.0, 0.0, 1.0]   \n","1    test/patient64742/study1/view1_frontal.jpg            [0.0, 0.0, 1.0]   \n","2    test/patient64743/study1/view1_frontal.jpg            [0.0, 0.0, 1.0]   \n","3    test/patient64744/study1/view1_frontal.jpg            [0.0, 1.0, 0.0]   \n","4    test/patient64744/study1/view2_lateral.jpg            [0.0, 1.0, 0.0]   \n","..                                          ...                        ...   \n","663  test/patient65236/study1/view1_frontal.jpg            [0.0, 0.0, 1.0]   \n","664  test/patient65237/study1/view1_frontal.jpg            [0.0, 0.0, 1.0]   \n","665  test/patient65238/study1/view1_frontal.jpg            [0.0, 1.0, 0.0]   \n","666  test/patient65239/study1/view1_frontal.jpg            [0.0, 0.0, 1.0]   \n","667  test/patient65240/study1/view1_frontal.jpg            [0.0, 1.0, 0.0]   \n","\n","        Cardiomegaly     Lung Opacity      Lung Lesion            Edema  \\\n","0    [0.0, 0.0, 1.0]  [0.0, 0.0, 1.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]   \n","1    [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]   \n","2    [0.0, 0.0, 1.0]  [0.0, 0.0, 1.0]  [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]   \n","3    [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]   \n","4    [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]   \n","..               ...              ...              ...              ...   \n","663  [0.0, 0.0, 1.0]  [0.0, 0.0, 1.0]  [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]   \n","664  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]   \n","665  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]   \n","666  [0.0, 0.0, 1.0]  [0.0, 0.0, 1.0]  [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]   \n","667  [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]   \n","\n","       Consolidation        Pneumonia      Atelectasis     Pneumothorax  \\\n","0    [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]   \n","1    [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]  [0.0, 1.0, 0.0]   \n","2    [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]  [0.0, 1.0, 0.0]   \n","3    [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]   \n","4    [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]   \n","..               ...              ...              ...              ...   \n","663  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]   \n","664  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]   \n","665  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]   \n","666  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]  [0.0, 1.0, 0.0]   \n","667  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]  [0.0, 1.0, 0.0]   \n","\n","    Pleural Effusion    Pleural Other         Fracture  Support Devices  \n","0    [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]  \n","1    [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  \n","2    [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  \n","3    [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]  \n","4    [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]  \n","..               ...              ...              ...              ...  \n","663  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]  \n","664  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  \n","665  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]  \n","666  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]  \n","667  [0.0, 0.0, 1.0]  [0.0, 1.0, 0.0]  [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0]  \n","\n","[668 rows x 14 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["labels = ['Pleural Effusion', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Edema', 'Pleural Other', 'Fracture', 'Support Devices']\n","\n","test_df = pd.read_csv(\"../data/processed/test_one_hot_encoded.csv\", index_col=False)\n","test_df[\"Path\"] = test_df[\"Path\"].apply(fix_error_paths)\n","\n","for label in labels:\n","    test_df[label] = test_df[label].apply(str_to_array)\n","\n","display(test_df)"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Dataset implementation"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# Implementation of Custom Dataset Class for CheXPhoto Dataset\n","class CheXDataset(Dataset):\n","    # Accepts dataframe object and str\n","    def __init__(self, df: pd.DataFrame, px_size: int = 256):\n","        self.dataframe = df.copy()\n","        self.px_size = px_size\n","        self.transform = T.Compose([\n","            v2.Resize((self.px_size, self.px_size), interpolation=T.InterpolationMode.BICUBIC)\n","        ])\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        x_path = data_path + \"/\" + self.dataframe.iloc[idx, 0].split(\"CheXphoto-v1.0\", 1)[-1]\n","        resized_x_tensor = self.transform(read_image(x_path, mode = ImageReadMode.RGB)) /255\n","        y = torch.tensor(self.dataframe.iloc[idx, 1]).type(torch.LongTensor)\n","        return resized_x_tensor, y"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def preprocess_pEffClassification(df):\n","    df.drop(columns=['Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Edema', 'Pleural Other', 'Fracture', 'Support Devices'], axis=1, inplace=True)\n","    return df\n","\n","def preprocess_cardioClassification(df):\n","    df.drop(columns=['Enlarged Cardiomediastinum', 'Pleural Effusion', 'Lung Opacity', 'Lung Lesion', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Edema', 'Pleural Other', 'Fracture', 'Support Devices'], axis=1, inplace=True)\n","    return df\n","\n","def ohe_to_class(row):\n","    if np.sum(row) > 0:\n","        return np.argmax(row)\n","    else:\n","        return -100\n","\n","# Creates copy of train_df for pEff and cardio\n","test_p = test_df.copy()\n","test_c = test_df.copy()"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Path</th>\n","      <th>Pleural Effusion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>test/patient64741/study1/view1_frontal.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>test/patient64742/study1/view1_frontal.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>test/patient64743/study1/view1_frontal.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>test/patient64744/study1/view1_frontal.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>test/patient64744/study1/view2_lateral.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>663</th>\n","      <td>test/patient65236/study1/view1_frontal.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>664</th>\n","      <td>test/patient65237/study1/view1_frontal.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>665</th>\n","      <td>test/patient65238/study1/view1_frontal.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>666</th>\n","      <td>test/patient65239/study1/view1_frontal.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>667</th>\n","      <td>test/patient65240/study1/view1_frontal.jpg</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>668 rows × 2 columns</p>\n","</div>"],"text/plain":["                                           Path  Pleural Effusion\n","0    test/patient64741/study1/view1_frontal.jpg                 1\n","1    test/patient64742/study1/view1_frontal.jpg                 1\n","2    test/patient64743/study1/view1_frontal.jpg                 1\n","3    test/patient64744/study1/view1_frontal.jpg                 1\n","4    test/patient64744/study1/view2_lateral.jpg                 1\n","..                                          ...               ...\n","663  test/patient65236/study1/view1_frontal.jpg                 1\n","664  test/patient65237/study1/view1_frontal.jpg                 1\n","665  test/patient65238/study1/view1_frontal.jpg                 1\n","666  test/patient65239/study1/view1_frontal.jpg                 1\n","667  test/patient65240/study1/view1_frontal.jpg                 2\n","\n","[668 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["# Drop all other labels\n","pEff_test = preprocess_pEffClassification(test_p)\n","# Replace OHE with int class labels\n","pEff_test[\"Pleural Effusion\"] = pEff_test[\"Pleural Effusion\"].apply(ohe_to_class)\n","display(pEff_test)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Path</th>\n","      <th>Cardiomegaly</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>test/patient64741/study1/view1_frontal.jpg</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>test/patient64742/study1/view1_frontal.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>test/patient64743/study1/view1_frontal.jpg</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>test/patient64744/study1/view1_frontal.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>test/patient64744/study1/view2_lateral.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>663</th>\n","      <td>test/patient65236/study1/view1_frontal.jpg</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>664</th>\n","      <td>test/patient65237/study1/view1_frontal.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>665</th>\n","      <td>test/patient65238/study1/view1_frontal.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>666</th>\n","      <td>test/patient65239/study1/view1_frontal.jpg</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>667</th>\n","      <td>test/patient65240/study1/view1_frontal.jpg</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>668 rows × 2 columns</p>\n","</div>"],"text/plain":["                                           Path  Cardiomegaly\n","0    test/patient64741/study1/view1_frontal.jpg             2\n","1    test/patient64742/study1/view1_frontal.jpg             1\n","2    test/patient64743/study1/view1_frontal.jpg             2\n","3    test/patient64744/study1/view1_frontal.jpg             1\n","4    test/patient64744/study1/view2_lateral.jpg             1\n","..                                          ...           ...\n","663  test/patient65236/study1/view1_frontal.jpg             2\n","664  test/patient65237/study1/view1_frontal.jpg             1\n","665  test/patient65238/study1/view1_frontal.jpg             1\n","666  test/patient65239/study1/view1_frontal.jpg             2\n","667  test/patient65240/study1/view1_frontal.jpg             1\n","\n","[668 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["# Drop all other labels\n","cardio_test = preprocess_cardioClassification(test_c)\n","# Replace OHE with int class labels\n","cardio_test[\"Cardiomegaly\"] = cardio_test[\"Cardiomegaly\"].apply(ohe_to_class)\n","display(cardio_test)"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Dataloader"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["# Load into custom Dataset\n","pEff_test_data = CheXDataset(pEff_test)\n","cardio_test_data = CheXDataset(cardio_test)\n","\n","# Load into DataLoader\n","batch_size = 128\n","pEff_test_loader = DataLoader(pEff_test_data, batch_size)\n","cardio_test_loader = DataLoader(cardio_test_data, batch_size)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[0.0275, 0.0314, 0.0235,  ..., 0.0314, 0.0314, 0.0314],\n","         [0.0157, 0.0235, 0.0118,  ..., 0.0314, 0.0314, 0.0314],\n","         [0.0118, 0.0235, 0.0235,  ..., 0.0196, 0.0314, 0.0314],\n","         ...,\n","         [0.0824, 0.1176, 0.1961,  ..., 0.2314, 0.1569, 0.2588],\n","         [0.0824, 0.1294, 0.2078,  ..., 0.2314, 0.1686, 0.2784],\n","         [0.0824, 0.1451, 0.2196,  ..., 0.2392, 0.1725, 0.2471]],\n","\n","        [[0.0275, 0.0314, 0.0235,  ..., 0.0314, 0.0314, 0.0314],\n","         [0.0157, 0.0235, 0.0118,  ..., 0.0314, 0.0314, 0.0314],\n","         [0.0118, 0.0235, 0.0235,  ..., 0.0196, 0.0314, 0.0314],\n","         ...,\n","         [0.0824, 0.1176, 0.1961,  ..., 0.2314, 0.1569, 0.2588],\n","         [0.0824, 0.1294, 0.2078,  ..., 0.2314, 0.1686, 0.2784],\n","         [0.0824, 0.1451, 0.2196,  ..., 0.2392, 0.1725, 0.2471]],\n","\n","        [[0.0275, 0.0314, 0.0235,  ..., 0.0314, 0.0314, 0.0314],\n","         [0.0157, 0.0235, 0.0118,  ..., 0.0314, 0.0314, 0.0314],\n","         [0.0118, 0.0235, 0.0235,  ..., 0.0196, 0.0314, 0.0314],\n","         ...,\n","         [0.0824, 0.1176, 0.1961,  ..., 0.2314, 0.1569, 0.2588],\n","         [0.0824, 0.1294, 0.2078,  ..., 0.2314, 0.1686, 0.2784],\n","         [0.0824, 0.1451, 0.2196,  ..., 0.2392, 0.1725, 0.2471]]]) torch.Size([3, 256, 256]) torch.float32\n","tensor(1) torch.Size([]) torch.int64\n"]}],"source":["x, y = pEff_test_data[0]\n","print(x, x.shape, x.dtype)\n","print(y, y.shape, y.dtype)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[0.0275, 0.0314, 0.0235,  ..., 0.0314, 0.0314, 0.0314],\n","         [0.0157, 0.0235, 0.0118,  ..., 0.0314, 0.0314, 0.0314],\n","         [0.0118, 0.0235, 0.0235,  ..., 0.0196, 0.0314, 0.0314],\n","         ...,\n","         [0.0824, 0.1176, 0.1961,  ..., 0.2314, 0.1569, 0.2588],\n","         [0.0824, 0.1294, 0.2078,  ..., 0.2314, 0.1686, 0.2784],\n","         [0.0824, 0.1451, 0.2196,  ..., 0.2392, 0.1725, 0.2471]],\n","\n","        [[0.0275, 0.0314, 0.0235,  ..., 0.0314, 0.0314, 0.0314],\n","         [0.0157, 0.0235, 0.0118,  ..., 0.0314, 0.0314, 0.0314],\n","         [0.0118, 0.0235, 0.0235,  ..., 0.0196, 0.0314, 0.0314],\n","         ...,\n","         [0.0824, 0.1176, 0.1961,  ..., 0.2314, 0.1569, 0.2588],\n","         [0.0824, 0.1294, 0.2078,  ..., 0.2314, 0.1686, 0.2784],\n","         [0.0824, 0.1451, 0.2196,  ..., 0.2392, 0.1725, 0.2471]],\n","\n","        [[0.0275, 0.0314, 0.0235,  ..., 0.0314, 0.0314, 0.0314],\n","         [0.0157, 0.0235, 0.0118,  ..., 0.0314, 0.0314, 0.0314],\n","         [0.0118, 0.0235, 0.0235,  ..., 0.0196, 0.0314, 0.0314],\n","         ...,\n","         [0.0824, 0.1176, 0.1961,  ..., 0.2314, 0.1569, 0.2588],\n","         [0.0824, 0.1294, 0.2078,  ..., 0.2314, 0.1686, 0.2784],\n","         [0.0824, 0.1451, 0.2196,  ..., 0.2392, 0.1725, 0.2471]]]) torch.Size([3, 256, 256]) torch.float32\n","tensor(2) torch.Size([]) torch.int64\n"]}],"source":["x, y = cardio_test_data[0]\n","print(x, x.shape, x.dtype)\n","print(y, y.shape, y.dtype)"]},{"cell_type":"markdown","metadata":{},"source":["### Second iteration - Basic Convolutional neural network (CNN)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["class ConvolutionBlock(nn.Module):\n","    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: int, pool_size: int = 2):\n","        super(ConvolutionBlock, self).__init__()\n","        \n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.activation = nn.ReLU()\n","        self.pooling = nn.MaxPool2d(pool_size, pool_size)\n","    \n","    def forward(self, x):\n","        out = self.conv(x)\n","        out = self.bn(out)\n","        out = self.activation(out)\n","        out = self.pooling(out)\n","        return out"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["class FCBlock(nn.Module):\n","    def __init__(self, in_features: int, out_features: int, dropout_rate: int=0.2):\n","        super(FCBlock, self).__init__()\n","        \n","        self.fc = nn.Linear(in_features, out_features, dtype=torch.float32)\n","        self.activation = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout_rate)\n","    \n","    def forward(self, x):\n","        out = self.fc(x)\n","        out = self.activation(out)\n","        out = self.dropout(out)\n","        return out"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["class ClassifierBlock(nn.Module):\n","    def __init__(self, in_features: int, hidden_features: tuple[int], out_features: int, dropout_rate: int = 0.2):\n","        super(ClassifierBlock, self).__init__()\n","\n","        self.inputs = FCBlock(in_features, hidden_features[0], dropout_rate)\n","        self.layers = nn.Sequential(*[FCBlock(hidden_features[i], hidden_features[i+1], dropout_rate) for i in range(len(hidden_features)-1)])\n","        self.linear = nn.Linear(hidden_features[-1], out_features, dtype=torch.float32)\n","    \n","    def forward(self, x):\n","        out = self.inputs(x)\n","        out = self.layers(out)\n","        out = self.linear(out)\n","        return out"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["class ConvolutionalNN(nn.Module):\n","    def __init__(self, channels: tuple[int], img_size: tuple[int], hidden_size: tuple[int], output_size: int, dropout_rate: int = 0.2):\n","        super(ConvolutionalNN, self).__init__()\n","        \n","        # Hyperparameters for Convolutional Layers\n","        self.kernel_size = 3\n","        self.stride = 1\n","        self.padding = 1\n","        self.pool_size = 2\n","\n","        # Input size of Image\n","        self.input_height, self.input_width = img_size\n","\n","        # Calculate Input Size of Classifier\n","        for i in range(len(channels) - 1):\n","            self.input_height = math.floor((self.input_height + 2 * self.padding - self.kernel_size)/ self.stride + 1) // self.pool_size\n","            self.input_width =  math.floor((self.input_width  + 2 * self.padding - self.kernel_size)/ self.stride + 1) // self.pool_size\n","        \n","        self.classifier_size = channels[-1] * self.input_height * self.input_width\n","        \n","        # Model Layers\n","        self.conv_layers = nn.Sequential(*[ConvolutionBlock(channels[i], channels[i+1], self.kernel_size, self.stride, self.padding, self.pool_size) for i in range(len(channels)-1)])\n","        self.classifier1 = ClassifierBlock(self.classifier_size, hidden_size, output_size, dropout_rate)\n","\n","    def forward(self,x):\n","        # Convolutional Layers\n","        out1 = self.conv_layers(x)\n","\n","        # keep batch size and flatten the rest\n","        out2 = out1.view(out1.size(0), -1)\n","\n","        # Classifier Layers\n","        out3 = self.classifier1(out2)\n","        \n","        return out3"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["from tqdm.notebook import tnrange, tqdm\n","from torchmetrics.classification import MulticlassAccuracy\n","\n","def test_loop(model, test_loader, loss):\n","    model.eval()\n","\n","    test_loss = 0\n","    test_total = 0\n","    test_accuracy = 0\n","\n","    accuracy = MulticlassAccuracy(num_classes=3, ignore_index=-100).to(device)\n","\n","    with torch.no_grad():\n","        for inputs, outputs in tqdm(test_loader):\n","            inputs_re, outputs_re = inputs.to(device), outputs.to(device)\n","            preds = model(inputs_re)\n","\n","            # Compute loss\n","            loss_value = loss(preds, outputs_re)\n","\n","            # Compute metrics\n","            test_loss += loss_value.item() * outputs_re.size(0)\n","            test_total += outputs_re.size(0)\n","\n","            accuracy.update(preds, outputs_re)\n","\n","    test_loss /= test_total\n","    test_accuracy = accuracy.compute()\n","\n","    print(f'--- Val Loss: {test_loss:.4f}, Train accuracy: {test_accuracy:.4f}\\n')\n","\n","    return test_loss, test_accuracy"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["def test(model, weights, test_loader):\n","    # loss function\n","    loss = nn.CrossEntropyLoss(weight=torch.tensor(weights), ignore_index=-100).to(device)\n","        \n","    # Test loop\n","    test_loss, test_accuracy = test_loop(model, test_loader, loss)\n","\n","    print(f\"Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["# Simplistic one for now, if need the full checkpoint, refer to helpers.py\n","def restore_model(model, path='../bin'):\n","    model_path = os.path.join(path, '{}-.ckpt')\n","    model.load_state_dict(torch.load(model_path)['model_state_dict'])"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"restore_model() missing 1 required positional argument: 'path'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[43], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m      8\u001b[0m model_pEff \u001b[38;5;241m=\u001b[39m ConvolutionalNN(channels, input_size, hidden_size, output_size, dropout_rate)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mrestore_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_pEff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_pEff)\n","\u001b[1;31mTypeError\u001b[0m: restore_model() missing 1 required positional argument: 'path'"]}],"source":["# Create Model\n","channels = (3, 8, 16, 32, 128)\n","input_size = (256, 256)\n","hidden_size = (8192, 512) \n","output_size = 3\n","dropout_rate = 0.2\n","\n","model_pEff = ConvolutionalNN(channels, input_size, hidden_size, output_size, dropout_rate).to(device)\n","restore_model(model_pEff, \"./bin/pEff-.ckpt\")\n","print(model_pEff)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5393b2e9ad834c45b1ee6f4236f59391","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["--- Val Loss: 1.1087, Train accuracy: 0.5000\n","\n","Test loss: 1.1087, Test accuracy: 0.5000\n"]}],"source":["pEff_weights = [1873/19582, 4976/19582, 12733/19582]\n","test(model_pEff)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ConvolutionalNN(\n","  (conv_layers): Sequential(\n","    (0): ConvolutionBlock(\n","      (conv): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activation): ReLU()\n","      (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (1): ConvolutionBlock(\n","      (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activation): ReLU()\n","      (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (2): ConvolutionBlock(\n","      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activation): ReLU()\n","      (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","    (3): ConvolutionBlock(\n","      (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activation): ReLU()\n","      (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    )\n","  )\n","  (classifier1): ClassifierBlock(\n","    (inputs): FCBlock(\n","      (fc): Linear(in_features=32768, out_features=8192, bias=True)\n","      (activation): ReLU()\n","      (dropout): Dropout(p=0.2, inplace=False)\n","    )\n","    (layers): Sequential(\n","      (0): FCBlock(\n","        (fc): Linear(in_features=8192, out_features=512, bias=True)\n","        (activation): ReLU()\n","        (dropout): Dropout(p=0.2, inplace=False)\n","      )\n","    )\n","    (linear): Linear(in_features=512, out_features=3, bias=True)\n","  )\n",")\n"]}],"source":["# Create Model\n","channels = (3, 8, 16, 32, 128)\n","input_size = (256, 256)\n","hidden_size = (8192, 512) \n","output_size = 3\n","dropout_rate = 0.2\n","\n","model_cardio = ConvolutionalNN(channels, input_size, hidden_size, output_size, dropout_rate).to(device)\n","restore_model(model_cardio, \"./bin/cardio-.ckpt\")\n","print(model_cardio)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95f0e60df8104c68af393480cee17fa7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["--- Val Loss: 1.0950, Train accuracy: 0.5000\n","\n","Test loss: 1.0950, Test accuracy: 0.5000\n"]}],"source":["cardio_weights = [1149/6812, 1623/6812, 4040/6812]\n","test(model_cardio, cardio_weights, cardio_test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"89c05e867478484fbace446920d90168","deepnote_persisted_session":{"createdAt":"2024-03-27T01:17:26.807Z"},"kernelspec":{"display_name":"env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
