{"cells":[{"cell_type":"markdown","metadata":{"id":"P7Wnon2hSjX4","cell_id":"76ab8bde33b542afb6d0413ffc6bb1f8","deepnote_cell_type":"markdown"},"source":"# 50.039 Theory and Practice of Deep Learning Project 2024","block_group":"af6c775bacfd4e01b495808e679ef6bb"},{"cell_type":"markdown","metadata":{"id":"8pDddTCFTJ5e","cell_id":"a4e06d30136d405fafb20a665bef99ba","deepnote_cell_type":"markdown"},"source":"Group 10\n- Issac Jose Ignatius (1004999)\n- Mahima Sharma (1006106)\n- Dian Maisara (1006377)\n","block_group":"7783b012fc4a40318d7c5499d935f773"},{"cell_type":"markdown","metadata":{"id":"vca9P0PG41JP","cell_id":"abba0498fc274b1dadd617faa0fd896d","deepnote_cell_type":"markdown"},"source":"### Import all relevant libraries","block_group":"e89b061a83c54a41b523daaa0ec9743a"},{"cell_type":"code","metadata":{"id":"zhwXTMVh444O","colab":{"base_uri":"https://localhost:8080/"},"outputId":"80c0b9fe-6f28-46b3-9f07-18b3fa99035d","source_hash":null,"execution_start":1711475524785,"execution_millis":6500,"deepnote_to_be_reexecuted":false,"cell_id":"443bbd1b51054c4b92c17f7c455a9cbb","deepnote_cell_type":"code"},"source":"# Matplotlib\n# import matplotlib.pyplot as plt\n# from matplotlib.lines import Line2D\n# Numpy\nimport numpy as np\n# Pandas\nimport pandas as pd\n# Torch\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision.transforms import ToTensor\nfrom torchvision.io import read_image\n\n!pip install torchmetrics\nfrom torchmetrics.classification import BinaryAccuracy","block_group":"0e5f647fed944f1191ae363149651d78","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting torchmetrics\n  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting lightning-utilities>=0.8.0\n  Downloading lightning_utilities-0.11.1-py3-none-any.whl (26 kB)\nRequirement already satisfied: torch>=1.10.0 in /root/venv/lib/python3.10/site-packages (from torchmetrics) (2.2.1)\nRequirement already satisfied: packaging>17.1 in /shared-libs/python3.10/py-core/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: numpy>1.20.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from torchmetrics) (1.23.4)\nRequirement already satisfied: typing-extensions in /root/venv/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.10.0)\nRequirement already satisfied: setuptools in /root/venv/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.10/py-core/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.0.9)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /root/venv/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/venv/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/venv/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/venv/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/venv/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\nRequirement already satisfied: networkx in /root/venv/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/venv/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\nRequirement already satisfied: triton==2.2.0 in /root/venv/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/venv/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\nRequirement already satisfied: fsspec in /shared-libs/python3.10/py/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.2.0)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/venv/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/venv/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/venv/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\nRequirement already satisfied: jinja2 in /shared-libs/python3.10/py-core/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/venv/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\nRequirement already satisfied: sympy in /shared-libs/python3.10/py/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.11.1)\nRequirement already satisfied: filelock in /shared-libs/python3.10/py/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.8.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /root/venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.99)\nRequirement already satisfied: MarkupSafe>=2.0 in /shared-libs/python3.10/py-core/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.0.0)\nRequirement already satisfied: mpmath>=0.19 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.2.1)\nInstalling collected packages: lightning-utilities, torchmetrics\nSuccessfully installed lightning-utilities-0.11.1 torchmetrics-1.3.2\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/680cb853-5472-4866-bfd1-bc6c841d9498"},{"cell_type":"markdown","metadata":{"id":"dYA9XwWzT1eq","cell_id":"47e025e5145149c8ae1cf2e44cafb7ce","deepnote_cell_type":"markdown"},"source":"## Motivation","block_group":"0b20e0978586453182666828b637d889"},{"cell_type":"markdown","metadata":{"id":"Jym9IzWeTp_t","cell_id":"99112ef0090e42838c2c20296a402afa","deepnote_cell_type":"markdown"},"source":"Chest radiography is an essential diagnostic tool used in medical imaging to visualise structures and organs within the chest cavity. It is crucial for diagnosing various respiratory and heart-related conditions. However, with the increased demand for radiological reports within shorter timeframes to detect and treat illnesses, there have been insufficient radiologists available to perform such tasks at scale. Therefore, automated chest radiograph interpretation could provide substantial benefits supporting large-scale screening and population health initiatives. Deep-learning algorithms can be used to bridge this gap. They have been used for image classification, anomaly detection, organ segmentation, and disease progression prediction.\n<br><br>\n\n*In this project, we aim to train a deep neural network to perform multi-label image classification on a wide array of chest radiograph images that exhibit various pathologies.*<br><br>\n\n\n\n---\n\n\n","block_group":"9e7a4d30795c46c488071732bc67aeb0"},{"cell_type":"markdown","metadata":{"id":"Z12EVXOwUKsx","cell_id":"de19c43817134aa585a2812af88a3944","deepnote_cell_type":"markdown"},"source":"## Data Exploration","block_group":"b9dea532882e4ec69cfbc86c02b5beda"},{"cell_type":"markdown","metadata":{"id":"Zr9mlGP5UNb_","cell_id":"6708413788da4cf99258ee610025a356","deepnote_cell_type":"markdown"},"source":"The training and validation datasets are from the **CheXphoto dataset** (Philips et al., 2020). <br><br> CheXphoto comprises a training set of natural photos and synthetic transformations of 10,507 X-ray images from 3,000 unique patients (32,521 data points) sampled at random from the CheXpert training dataset and an accompanying validation set of natural and synthetic transformations applied to all 234 X-ray images from 200 patients with an additional 200 cell phone photos of x-ray films from another 200 unique patients (952 data points).","block_group":"ec017d3a929a43d993d2eb81e8fa3037"},{"cell_type":"markdown","metadata":{"id":"4t_Fw1lG_ouA","cell_id":"5863ff63682e4999a8c3f4132605b5c4","deepnote_cell_type":"markdown"},"source":"### DONT DELETE!!! Retrieving dataset from Google Cloud Storage (GCS)\n\n\n","block_group":"215b8072eb6842f8b8e33e46036c36f9"},{"cell_type":"code","metadata":{"id":"7VPAB3GgF5n3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"25ef1be2-c583-4cd2-d89f-4f1709c50179","source_hash":null,"execution_start":1711133776362,"execution_millis":278,"deepnote_to_be_reexecuted":false,"cell_id":"8f1186990a874ba2ac5b969bba488d06","deepnote_cell_type":"code"},"source":"#OLD CODE : not in use as we are bringing in training datasets from notebook 1\n# Connect to GCS to access data\n#from google.colab import auth\n#auth.authenticate_user() # TODO: everyone to send me gmail so I can have you authed for bucket access\n\n#project_id = 'tpdl-414711'\n#bucket_name = 'chexphoto-v1'\n#!gcloud config set project {project_id}\n\n# Install Cloud Storage FUSE.\n#!echo \"deb https://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n#!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n#!apt -qq update && apt -qq install gcsfuse\n\n# Mount a Cloud Storage bucket or location, without the gs:// prefix.\n#mount_path = \"chexphoto-v1\"  # or a location like \"my-bucket/path/to/mount\"\n#local_path = f\"/mnt/gs/{mount_path}\"\n\n#!mkdir -p {local_path}\n#!gcsfuse --implicit-dirs {mount_path} {local_path}","block_group":"6b05910f5b62431aad3229aa4e5183a3","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"04b8bccdc2f6420399a9b12ae18dcfa9","deepnote_cell_type":"code"},"source":"#!ls /datasets/chexphoto-v1/\n\n#local_path = \"/datasets/chexphoto-v1/\"","block_group":"732a781a22af4427b462cb3481b871c4","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"5UoygSwiKB48","cell_id":"5e3966f4e82140bdac6364c5577dba47","deepnote_cell_type":"markdown"},"source":"### Loading dataset (image and labels)","block_group":"c99e855f4e1f4ad0b49c0e4193a14ae1"},{"cell_type":"code","metadata":{"id":"fiNnq1xv_T-U","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c4d62638-d19b-4598-80fa-497312e52005","source_hash":null,"execution_start":1711133776363,"execution_millis":277,"deepnote_to_be_reexecuted":false,"cell_id":"6bf3ce4c203b4e8ab0ccce00c3f35de5","deepnote_cell_type":"code"},"source":"# Testing image loader shd work! as of 21/2/2024 at 3:35am\n\n#hardcode_image = local_path + \"/validation/valid/film/VBSF00001/study1/view1_frontal.jpg\"\n\n#x = read_image(hardcode_image)\n\n#print(f\"Tensor image: {x}\")\n","block_group":"c17a578026b944e58bcb22183c10792a","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"id":"lbTSCRYiKBR_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a2608d03-a51a-40c1-de63-42d6030c2bfb","source_hash":null,"execution_start":1711133776444,"execution_millis":701,"deepnote_to_be_reexecuted":false,"cell_id":"3260f45eb8f441ecbd42999a36a03ee8","deepnote_cell_type":"code"},"source":"#OLD CODE not in use \n# Testing excel loader (shd work! as of 21/2/2024 at 3:35am)\nhardcode_excel = local_path + \"/validation/valid.csv\"\n\nclass CheXDataset(torch.utils.data.Dataset):\n    def __init__(self):\n        self.dataframe = pd.read_csv(hardcode_excel)\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        # I know this isnt the most accurate just for the purpose of seeing if I can even load the data....\n        image_path = self.dataframe.iloc[idx, 0]\n        sex = self.dataframe.iloc[idx, 1]\n        age = self.dataframe.iloc[idx, 2]\n        FoL = self.dataframe.iloc[idx, 3]\n        AoP = self.dataframe.iloc[idx, 4]\n        y = torch.tensor(self.dataframe.iloc[idx, 5:], dtype=torch.float64)\n        return [image_path, sex, age, FoL, AoP], y\n\n\ncheX_data = CheXDataset()\n[ path, sex, age, FoL, AoP ], y = cheX_data[787] # shd correspond with image loaded above (VBSF00001)\nprint(f\"Non-tensor values: Image Path: {path} Sex: {sex} Age: {age} FoL ? {FoL} AoP? {AoP}\")\nprint(f\"Tensor labels: {y}\")","block_group":"9e2c3f1b6a91480abb97742f975b16a2","execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'local_path' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#OLD CODE not in use \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Testing excel loader (shd work! as of 21/2/2024 at 3:35am)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m hardcode_excel \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_path\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/validation/valid.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCheXDataset\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n","\u001b[0;31mNameError\u001b[0m: name 'local_path' is not defined"]}],"outputs_reference":"s3:deepnote-cell-outputs-production/6ac3deea-901d-42d6-848f-3e6a330e6423"},{"cell_type":"markdown","metadata":{"id":"j2wt_vLGW7us","cell_id":"bf0a3a5c848b4f479b857e07336ad6e5","deepnote_cell_type":"markdown"},"source":"## Data Preprocessing","block_group":"c7772bf8603742d197fb01fa22dd7d01"},{"cell_type":"markdown","metadata":{"id":"GRIQXus-aYux","cell_id":"03eb3f37690f4f99aba6bf036b50a60e","deepnote_cell_type":"markdown"},"source":"For our task, we would need to transform the inputs and labels into a more appropriate form using X and one-hot encoding. This is to ensure \\<insert justification here\\>","block_group":"97ba3391c8eb4cc2bfbfc0d83f6b9485"},{"cell_type":"markdown","metadata":{"id":"V_IGOXZVZE3K","cell_id":"4b6934153d044473a70920679e3b553b","deepnote_cell_type":"markdown"},"source":"### If we need to do anything to the images (greymap conversion etc.), do it here (remove if N/A)","block_group":"248dd90d4edd46f89302102f333e6ce9"},{"cell_type":"code","metadata":{"id":"VxV8iI7mZLv1","source_hash":null,"execution_start":1711083160402,"execution_millis":22,"deepnote_to_be_reexecuted":true,"cell_id":"70aeb7fcbc574404b50e48f72ac34ec6","deepnote_cell_type":"code"},"source":"# Implementation of custom Dataset \nclass CheXDataset(torch.utils.data.Dataset):\n    def __init__(self, df): #previously csv_path but after data preprocessing, we can accept directly\n        self.dataframe = df #pd.read_csv(csv_path)\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        x_path = LOCAL_PATH + \"/\" + self.dataframe.iloc[idx, 0].split(\"CheXphoto-v1.0\", 1)[-1]\n        x_tensor =  read_image(x_path) / 255\n        y = torch.tensor(self.dataframe.iloc[idx, 5:], dtype=torch.float64)\n        return [x_path, x_tensor], y # sex, age, FoL, AoP is removed for now","block_group":"2ad838c939b34706b7b39c71b7d7a3a3","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1711083237391,"execution_millis":41,"deepnote_to_be_reexecuted":true,"cell_id":"df20610390874cfb90cf07797eac1cca","deepnote_cell_type":"code"},"source":"# Create train, test and valid datasets using CheXDataset\n# labels = [LOCAL_PATH +\"/train.csv\", LOCAL_PATH +\"/test.csv\", LOCAL_PATH +\"/valid.csv\"]\ncheX_train_data = CheXDataset(train_df3) #CheXDataset(labels[0])\ncheX_valid_data = CheXDataset(valid_df3) #CheXDataset(labels[2])","block_group":"4ec55cf7c2f54b9a99e85cd58dd90721","execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_df3' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create train, test and valid datasets using CheXDataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# labels = [LOCAL_PATH +\"/train.csv\", LOCAL_PATH +\"/test.csv\", LOCAL_PATH +\"/valid.csv\"]\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m cheX_train_data \u001b[38;5;241m=\u001b[39m CheXDataset(\u001b[43mtrain_df3\u001b[49m) \u001b[38;5;66;03m#CheXDataset(labels[0])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m cheX_test_data \u001b[38;5;241m=\u001b[39m CheXDataset(test_df3) \u001b[38;5;66;03m#CheXDataset(labels[1])\u001b[39;00m\n\u001b[1;32m      5\u001b[0m cheX_valid_data \u001b[38;5;241m=\u001b[39m CheXDataset(valid_df3)\n","\u001b[0;31mNameError\u001b[0m: name 'train_df3' is not defined"]}],"outputs_reference":"s3:deepnote-cell-outputs-production/94486de0-f90f-4ab6-a5e8-5ce82fbb3b08"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1711083349915,"execution_millis":70,"deepnote_to_be_reexecuted":true,"cell_id":"045d8ac6b1e94512a605f9ec45427449","deepnote_cell_type":"code"},"source":"# Retrieve train sample using custom Dataset above\n[ image, x ], y = cheX_train_data[10]\n\n# Print out values and display image\nprint(f\"Non-tensor values: Image Path: {image}\\n\")\nprint(f\" Tensor image: {x} Tensor labels: {y}\\n\")\nprint(f\" Tensor image: {x*255} Tensor labels: {y}\\n\")\npil_img = Image(image)\ndisplay(pil_img)\n\n# Retrieve valid sample using custom Dataset above\n[ image, x ], y = cheX_valid_data[34]\n\n# Print out values and display image\nprint(f\"Non-tensor values: Image Path: {image}\\n\")\nprint(f\" Tensor image: {x} Tensor labels: {y}\\n\")\npil_img = Image(image)\ndisplay(pil_img)","block_group":"9672911f1c974c92b830fe26e97e1a6d","execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'cheX_train_data' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Retrieve train sample using custom Dataset above\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m [ image, x ], y \u001b[38;5;241m=\u001b[39m \u001b[43mcheX_train_data\u001b[49m[\u001b[38;5;241m10\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Print out values and display image\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-tensor values: Image Path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'cheX_train_data' is not defined"]}],"outputs_reference":"s3:deepnote-cell-outputs-production/5043d9b2-49d9-476b-b09a-4da0f381924c"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"7648a050adf74ee1b859c6dabf994475","deepnote_cell_type":"code"},"source":"","block_group":"995210782b3d4fc2a252130232781fb3","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"eea51b5eb82742f3a328fe49feab21e5","deepnote_cell_type":"code"},"source":"","block_group":"bf888a9725c644d0ba5b89155a8f88e9","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"dKvgB1awYvI3","cell_id":"d0c130dabc2f45fea3b8ac1c32a09c59","deepnote_cell_type":"markdown"},"source":"### One-hot encoding of labels","block_group":"97f136494148486ba1640bee53bd0f65"},{"cell_type":"markdown","metadata":{"id":"HgoKCekkUx12","cell_id":"c273cf4e9a9e48f7b8fd38adaa818c98","deepnote_cell_type":"markdown"},"source":"## Model Tuning","block_group":"f6ca88950f184272a9c779cc61092558"},{"cell_type":"markdown","metadata":{"id":"ez8gTQhpVdsO","cell_id":"718078bf47bb445381032c44da67fc8c","deepnote_cell_type":"markdown"},"source":"Our initial model is a simple feedforward neural network with multiple heads (14 heads) capable of classifying each observation for the various pathologies. We utilise the Cross-entropy loss function to optimise the model during training.\n\n**This is a TODO since it can change**\n","block_group":"944cb447555a4ee4a4a270924d91b89d"},{"cell_type":"markdown","metadata":{"id":"QQwIQXo-X989","cell_id":"31b146c0d9264786b18179542131665b","deepnote_cell_type":"markdown"},"source":"### First iteration - Simple feedforward neural network","block_group":"d2ab23250ea74f98af3204259e4caad5"},{"cell_type":"markdown","metadata":{"id":"L9EUOt_1Z3Q2","cell_id":"209c03519363493f99de3f954fe32abe","deepnote_cell_type":"markdown"},"source":"Maybe add a description here how the multi-head was implemented (with sources)","block_group":"edab2083fbfd42f39767768c1d90bd93"},{"cell_type":"markdown","metadata":{"id":"wX3UD3CeYPqp","cell_id":"7b049a3b3fa44c63aa4a58f7a1998f1a","deepnote_cell_type":"markdown"},"source":"#### Model\n\n","block_group":"1e84ba53040d444d909596c4906a4c4f"},{"cell_type":"code","metadata":{"id":"c32N2YoBX6V0","source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"d0ba44dd9c6a4c7cb163451556057152","deepnote_cell_type":"code"},"source":"# Write out our base model here","block_group":"d41a9050e49448b1bf3a09d3aa68aebc","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"WDTZc-m2YULr","cell_id":"f6ea8eefc5fb4afe9689a0c8683b56a9","deepnote_cell_type":"markdown"},"source":"#### Training","block_group":"b51e1a78828540ceaed9109addef399d"},{"cell_type":"code","metadata":{"id":"gjotcljqYV-k","source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"c8ff85cb6194420ea8528d507156cf8d","deepnote_cell_type":"code"},"source":"","block_group":"53c6b0190c8a4f4aa35dd54b4598352a","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"h4dzJx1DYZFc","cell_id":"ab81a118e4994f338baf4efd3bb6ed79","deepnote_cell_type":"markdown"},"source":"#### Evaluation","block_group":"6b5ced37c6ea40c9bba6fc5f5e4622bc"},{"cell_type":"code","metadata":{"id":"RPorquhbYbkD","source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"838876c97888421caa86711eaad342a5","deepnote_cell_type":"code"},"source":"","block_group":"47893d5457904c2ab4094b5fb1934a7a","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"IZmg6BlqYfQy","cell_id":"f7d78259eb51433a84501cb27aa530a4","deepnote_cell_type":"markdown"},"source":"### Second iteration - Convolutional neural network (CNN)","block_group":"3b43627620084a63aea7f8a0186d334a"},{"cell_type":"markdown","metadata":{"id":"VLTx5JIrZ8gX","cell_id":"81cd3b7ce34d455c8471c9f572c1d494","deepnote_cell_type":"markdown"},"source":"Gradually, we moved the model into a traditional CNN-based architecture to see if we can surpass the performance from above. Briefly discuss what we needed to add to the model (filtering, convolution blablabla)","block_group":"887f79fc00b84b11a8a9ac03fd46f6e4"},{"cell_type":"markdown","metadata":{"id":"dyJAs1zJYiRw","cell_id":"e628f7a041c3412da4f737120a53b803","deepnote_cell_type":"markdown"},"source":"#### Model","block_group":"5d168861feb84fd98ce7e1bf66b7977c"},{"cell_type":"code","metadata":{"id":"8V4Db-KUYheY","source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"10a40549d65d41338d83ac21440d778a","deepnote_cell_type":"code"},"source":"","block_group":"5950498b2dd34db78d1cb01b048d9350","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"PWdhkE_QYm5q","cell_id":"28fa1bd52f6a4d1f94a252ecf5b9a4e7","deepnote_cell_type":"markdown"},"source":"#### Training","block_group":"60d443f7f7374058981efa293fcd4d12"},{"cell_type":"code","metadata":{"id":"xL8uRUdjYpC6","source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"39a4683d62cf453a824638c08311af29","deepnote_cell_type":"code"},"source":"","block_group":"4c81e0f1c170490dbefb0664f2698fb1","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"ehxyUaqDYpU5","cell_id":"afd1fe4fadb548a3bb07e2481c8861d2","deepnote_cell_type":"markdown"},"source":"#### Evaluation","block_group":"cd4f2c12a25741f8a75ce7abac890bdb"},{"cell_type":"code","metadata":{"id":"yfEFs67rYrwb","source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"ace02b86307743128b4c8b618bbd16f3","deepnote_cell_type":"code"},"source":"","block_group":"a4fbfa8048dd4cbe883c3e0b2f3c5f36","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"iI5olU_kZFMt","cell_id":"19d80787d418467a8828ff94da6dd8f3","deepnote_cell_type":"markdown"},"source":"## Observations","block_group":"8d52eb615ab14c99a3ee53e6bf7567a0"},{"cell_type":"markdown","metadata":{"id":"cWoXsxGDZSzV","cell_id":"89a1f951eb5a49a5b03857d00a668997","deepnote_cell_type":"markdown"},"source":"**TODO** Discuss whether its right for us to pluck all our evaluation and training together and discuss it here or break up the code without any descriptions\n","block_group":"aa0b10a578c04372a7aa0413e0d1e397"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d056a7b8-1929-4f43-a228-a643b0e765c5' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2024-03-27T01:17:26.807Z"},"deepnote_notebook_id":"89c05e867478484fbace446920d90168","deepnote_execution_queue":[]}}